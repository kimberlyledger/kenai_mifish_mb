---
title: "PCR replicate decontamination and biological replicate summary"
author: "Kimberly Ledger"
date: "2024-10-04"
output: html_document
---

bioinformatic decontamination of kenia mifish metabarcoding asvs and replicates

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

libraries
```{r}
library(tidyverse)
rename <- dplyr::rename
```

load sample metadata
```{r}
sample_metadata <- read.csv("/home/kimberly.ledger/kenai_mifish_mb/metadata_Kenai_Kasilof_2022_fish.csv")
```

check sequence table outputs
```{r}
asv_table <- readRDS("/home/kimberly.ledger/kenai_mifish_mb/kenai_20231027/post/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
asv_table <- data.frame(t(asv_table))

#set column names to be ASV# 
colnames(asv_table) <- asv_table["ASV",]

#remove row that has ASV#
asv_table <- asv_table[!rownames(asv_table) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(asv_table)) {
  asv_table[[col]] <- as.numeric(asv_table[[col]])
}

#make make sample ID a column 
asv_table$sample_ID <- rownames(asv_table)
```


add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- sample_metadata %>%
  dplyr::select(!collection_date) %>%
  dplyr::select(!plate_num) %>%
  dplyr::select(!plate_well) %>%
  dplyr::select(!notes) %>%
  left_join(asv_table, by = "sample_ID")

# asv_table_with_sample_type <- asv_table %>%
#   mutate(sample_type = ifelse(str_starts(sample_ID, "H2O"), "pcr_blank", NA),
#          sample_type = ifelse(str_starts(sample_ID, "STURG"), "positive", sample_type),
#          sample_type = ifelse(str_detect(sample_ID, "^\\d"), "sample", sample_type),)

# make a variable for the first and last ASV column in the table
asv_first <- which(colnames(asv_table_with_sample_type) == "ASV_0001")
asv_last <- max(which(str_detect(colnames(asv_table_with_sample_type), "^ASV_")))
```

# account for likely contaminants 

- tag-jumping
- consider reads in the negative PCR controls
- consider reads in the field negatives

## Step 1. Account for tag-jumping by using the positive controls 

subtract the proportion of reads that jumped into the positive control samples from each environmental sample 

plot positives first
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  mutate(reads = ifelse(is.na(reads), 0, reads)) %>%
  filter(sample_type %in% c("positive")) %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - positive controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    #legend.position = "none",
    legend.title = element_blank()
  )
```

identify the maximum proportion of reads for each ASV found in the positive controls
```{r}
prop_asvs_in_positives <- asv_table_with_sample_type %>%
  filter(sample_type == "positive") %>%
  filter(!sample_ID %in% c("STURG1-C", "STURG2-C")) %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop))
prop_asvs_in_positives
```

ASVs 12, 44, and 76 are sturgeon. good. 
very low levels of tag-jumping. 

subtract the max proportion of tag-jumped reads for each ASV from samples
```{r}
indexhop_table <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  mutate(reads = ifelse(is.na(reads), 0, reads)) %>%
  group_by(sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads, na.rm = T)) %>%
  left_join(prop_asvs_in_positives, by = c("ASV")) %>%
  mutate(IndexHoppingReads = TotalReadsPerSample*max_prop) %>%
  mutate(reads_IndexHop_removed = reads - IndexHoppingReads) %>%
  mutate(reads_IndexHop_removed = if_else(reads_IndexHop_removed < 0, 0, reads_IndexHop_removed))
head(indexhop_table)
```

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter1 <- indexhop_table %>%
  dplyr::select(sample_ID, sample_type, ASV, reads_IndexHop_removed) %>%
  dplyr::rename(reads = reads_IndexHop_removed)
```

this is a summary of the number of reads removed by ASV and sample_ID
```{r}
decontaminated_1 <- indexhop_table %>%
  dplyr::select(sample_ID, sample_type, ASV, IndexHoppingReads) %>%
  filter(sample_type == "sample") %>%
  group_by(ASV) %>%
  summarise(mean_reads = mean(IndexHoppingReads),
            reads_q.05 = quantile(IndexHoppingReads, probs=0.05),
            median_q.5 = median(IndexHoppingReads),
            reads_q.95 = quantile(IndexHoppingReads, probs=0.95)) %>%
  filter(mean_reads > 0)
decontaminated_1  
```

## Step 2. Remove ASVs only in controls and not in environmental samples and any ASVs that have more reads in controls than in environmental samples

number of reads
```{r}
reads_per_type_ASV <- asv_table_filter1 %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads, na.rm = TRUE)) %>%
  arrange(ASV)
```

what ASVs have no reads in field samples? 
```{r}
not_in_samples <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
    filter(sample == 0)
not_in_samples
```


what ASVs do have reads in samples, but more reads in the controls? 
```{r}
more_in_pcr_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(pcr_blank > sample)
head(more_in_pcr_blanks)

more_in_pc_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(positive > sample)
head(more_in_pc_blanks)

more_in_fb_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(field_blank > sample)
head(more_in_fb_blanks)
```

remove ASVs wtih no reads in samples
```{r}
asv_table_filter2 <- asv_table_filter1 %>%
  filter(!ASV %in% not_in_samples$ASV) #%>%
  #filter(!ASV %in% more_in_pcr_blanks$ASV) %>%
  #filter(!ASV %in% more_in_pc_blanks$ASV) #%>%
  #filter(!ASV %in% more_in_fb_blanks$ASV)
```


## Step 3. Remove ASVs without taxonomic ID 

now lets see how many of these ASVs have taxonomic IDs (these are not final tax ids)
```{r}
taxonomy <- read.csv("/home/kimberly.ledger/kenai_mifish_mb/outputs/taxonomy_collapsed.csv") %>%
  select(!X) %>%
  rename(ASV = qseqid)
```

fix an ASV that was missed during taxonomic assignment
```{r}
taxonomy <- taxonomy %>%
  bind_rows(
    taxonomy %>%
      filter(ASV == "ASV_0005") %>%
      mutate(ASV = "ASV_0039")
  )
```


```{r}
asv_table_filter2_with_tax <- asv_table_filter2 %>%
  left_join(taxonomy)
```

what ASV's do not have a taxonomic ID? 
```{r}
asv_table_filter2_with_tax %>%
  filter(is.na(taxon)) %>%
  group_by(ASV) %>%
  summarize(total_reads = sum(reads, na.rm = T))
```

remove ASVs with no taxonomic id - and remove any non-fish ASVs
```{r}
asv_table_filter3 <- asv_table_filter2_with_tax %>%
  filter(!is.na(taxon)) %>%
  filter(class != "Mammalia") %>%
  filter(kingdom != "Bacteria")
```


## Step 4. Consider what is still in the negative controls

```{r}
asv_table_filter3 %>%
  filter(sample_type %in% c("pcr_blank")) %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    #legend.position = "none",
    legend.title = element_blank()
  )
```

keep this at the asv level
```{r}
asvs_PCRN <- asv_table_filter3 %>%
  filter(sample_type %in% c("pcr_blank")) %>%
  group_by(sample_type, ASV) %>%
  summarise(total = sum(reads),
            max = max(reads),
            mean = mean(reads)) %>%
  arrange(desc(total)) %>%
  filter(total > 0)

asvs_PCRN
```

let sonia decide how to handle this. for now, I will NOT subtract any reads from samples based on the PCR negative controls. 
```{r}
# asvs_PCRN_mean <- asvs_PCRN %>%
#   select(!total) %>%
#   select(!max)
#   
# pcrn_table <- asv_table_filter3 %>%
#   left_join(asvs_PCRN_mean, by = c("seq_date", "ASV")) %>%
#   mutate(mean = ifelse(is.na(mean), 0, mean)) %>%
#   mutate(reads_pcrn_removed = reads - mean) %>%
#   mutate(reads_pcrn_removed = if_else(reads_pcrn_removed < 0, 0, reads_pcrn_removed))
# pcrn_table
```

clean up the table by removing columns no longer needed 
```{r}
# asv_table_filter4 <- pcrn_table %>%
#   select(!reads) %>%
#   select(!mean) %>%
#   dplyr::rename(reads = reads_pcrn_removed)
```


## Step 5.  Address field negatives. 

```{r}
#asv_table_filter4 %>%
asv_table_filter3 %>%
  filter(sample_type == "field_blank") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    #legend.position = "none",
    legend.title = element_blank()
  )
```

keep this at the asv level
```{r}
asvs_FN <- asv_table_filter3 %>%
  filter(sample_type %in% c("field_blank")) %>%
  group_by(sample_type, ASV) %>%
  summarise(total = sum(reads),
            max = max(reads),
            mean = mean(reads)) %>%
  arrange(desc(total)) %>%
  filter(total > 0)

asvs_FN
```

### Step 6. Consider taxa accumulation at the level of PCR replicates and remove PCR with low total read counts

```{r}
library(vegan)

taxon_table <- asv_table_filter3 %>%
  group_by(across(-c(ASV, reads))) %>%
  summarize(tot_reads = sum(reads))

taxon_table_wide <- taxon_table[,c(1,3,12)] %>%
  mutate(tot_reads = as.integer(tot_reads)) %>%
  pivot_wider(names_from = taxon, values_from = tot_reads)

sample_IDs <- taxon_table_wide$sample_ID
taxon_table_wide <- taxon_table_wide[,-1]

## plots the figure
rarecurve(taxon_table_wide, step = 20, col = "blue", label = FALSE, 
          main = "Sequencing Effort Curves",
          xlab = "Sequencing Depth", ylab = "Number of Species Identified",
          xlim = c(0,5000))
```


summarize in a table how many pcr replicates meet certain read count thresholds 
```{r}
read_summary <- taxon_table %>%
  group_by(sample_ID, sample_type) %>%
  summarize(reads = sum(tot_reads)) %>%
  arrange(desc(reads)) %>%
  group_by(sample_type) %>%
  summarize(atleast1 = sum(reads >= 1),
            atleast250 = sum(reads >= 250),
            atleast500 = sum(reads >= 500),
            atleast750 = sum(reads >= 750),
            atleast1k = sum(reads >= 1000))
```

based on taxa accumulation curve, we will remove any pcr replicate with fewer than 750 reads from downstream analyses

```{r}
reps_below <- asv_table_filter3 %>%
  group_by(sample_ID) %>%
  summarise(tot_reads = sum(reads)) %>%
  filter(tot_reads < 750)
```

```{r}
asv_table_filter4 <- asv_table_filter3 %>%
  filter(!sample_ID %in% reps_below$sample_ID)
```


### Step 6. Investigate dissimilarity between PCR replicates 

are there any samples that have made it to this point that don't actually have any reads? 
```{r}
asv_table_filter4 %>%
  group_by(sample_ID) %>%
  summarise(total_reads = sum(reads)) %>%
  arrange(total_reads)
```

how many pcr replicates does each extraction replicate have? 
```{r}
asv_table_filter4  %>%
  separate(sample_ID, into = c("bottle_ID", "replicate"), remove = F) %>%
  group_by(bottle_ID) %>%
  summarise(nrep = n_distinct(sample_ID)) %>%
  filter(nrep == 1)

asv_table_filter4  %>%
  separate(sample_ID, into = c("bottle_ID", "replicate"), remove = F) %>%
  group_by(bottle_ID) %>%
  summarise(nrep = n_distinct(sample_ID)) %>%
  filter(nrep == 2)

asv_table_filter4  %>%
  separate(sample_ID, into = c("bottle_ID", "replicate"), remove = F) %>%
  group_by(bottle_ID) %>%
  summarise(nrep = n_distinct(sample_ID)) %>%
  filter(nrep == 3)
```

cool. every remaining bottle sample has at least two pcr replicates with reads above filtering thresholds (and most have all three)

first, i'll calculate an eDNA index - going to do this at the TAXA level
```{r}
normalized <- asv_table_filter4 %>%
  group_by(across(-c(ASV, reads))) %>%
  summarise(tot_reads = sum(reads)) %>%
  ungroup() %>%
  group_by(sample_ID) %>%
  mutate(Tot = sum(tot_reads),
         Prop_reads = tot_reads/Tot) %>%
  ungroup() %>%
  dplyr::group_by(taxon) %>%
  mutate(Colmax = max(Prop_reads, na.rm = TRUE),
         Normalized_reads = Prop_reads/Colmax)

#add back in some metadata - will use this for dissimilarity measures
normalized <- normalized %>%
  left_join(sample_metadata) %>%
  unite(day_bottle, days_season, biological_ID, sep = "_", remove = FALSE) %>%
  unite(day_bottle_pcr, day_bottle, replicate, sep = "-", remove = FALSE)
```

```{r}
tibble_to_matrix <- function (tb) {
  
  tb %>%
  #normalized %>%
    #group_by(day_bottle_pcr, ASV) %>%
    group_by(day_bottle_pcr, taxon) %>% 
    summarise(nReads = sum(Normalized_reads)) %>% 
    #spread ( key = "ASV", value = "nReads", fill = 0) %>%
    spread ( key = "taxon", value = "nReads", fill = 0) %>%
    ungroup() -> matrix_1
    samples <- pull (matrix_1, day_bottle_pcr)
    matrix_1[,-1] -> matrix_1
    data.matrix(matrix_1) -> matrix_1
    dimnames(matrix_1)[[1]] <- samples
    vegdist(matrix_1) -> matrix_1
}

```

will run this seperatly for kenai and kasilof
```{r}
normalized_kenai <- normalized %>%
  filter(location == "Kenai")

all.distances.full <- tibble_to_matrix(normalized_kenai)

# Do all samples have a name?
summary(is.na(names(all.distances.full)))
```

make the pairwise distances a long table
```{r}
library(reshape)

as_tibble(subset(melt(as.matrix(all.distances.full)))) -> all.distances.melted

# Any major screw ups
summary(is.na(all.distances.melted$value))

# Now, create a three variables for all distances, they could be PCR replicates, BIOL replicates, or from the same site
all.distances.melted %>%
  separate (X1, into = "Bottle1", sep = "\\-", remove = FALSE) %>%
  separate (Bottle1, into = "Site1", remove = FALSE) %>%
  separate (X2, into ="Bottle2", sep = "\\-", remove = FALSE) %>%
  separate (Bottle2, into = "Site2", remove = FALSE) %>%
  mutate (Distance.type = case_when( Bottle1 == Bottle2 ~ "PCR Replicates",
                                      Site1 == Site2 ~ "Same Day",
                                      TRUE ~ "Different Day"
                                     )) %>%
  dplyr::select(Sample1 = X1, Sample2 = X2 , value , Distance.type) %>%
  filter (Sample1 != Sample2) -> all.distances.to.plot

# Checking all went well
sapply(all.distances.to.plot, function(x) summary(is.na(x)))
```

```{r}
all.distances.to.plot$Distance.type <- all.distances.to.plot$Distance.type  %>% fct_relevel("PCR Replicates", "Same Day")

ggplot (all.distances.to.plot) +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance",
        title = "Kenai") +
    guides (fill = "none")
```

```{r}
normalized_kasilof <- normalized %>%
  filter(location == "Kasilof")

all.distances.full <- tibble_to_matrix(normalized_kasilof)

# Do all samples have a name?
summary(is.na(names(all.distances.full)))
```

make the pairwise distances a long table
```{r}
library(reshape)

as_tibble(subset(melt(as.matrix(all.distances.full)))) -> all.distances.melted

# Any major screw ups
summary(is.na(all.distances.melted$value))

# Now, create a three variables for all distances, they could be PCR replicates, BIOL replicates, or from the same site
all.distances.melted %>%
  separate (X1, into = "Bottle1", sep = "\\-", remove = FALSE) %>%
  separate (Bottle1, into = "Site1", remove = FALSE) %>%
  separate (X2, into ="Bottle2", sep = "\\-", remove = FALSE) %>%
  separate (Bottle2, into = "Site2", remove = FALSE) %>%
  mutate (Distance.type = case_when( Bottle1 == Bottle2 ~ "PCR Replicates",
                                      Site1 == Site2 ~ "Same Day",
                                      TRUE ~ "Different Day"
                                     )) %>%
  dplyr::select(Sample1 = X1, Sample2 = X2 , value , Distance.type) %>%
  filter (Sample1 != Sample2) -> all.distances.to.plot

# Checking all went well
sapply(all.distances.to.plot, function(x) summary(is.na(x)))
```

```{r}
all.distances.to.plot$Distance.type <- all.distances.to.plot$Distance.type  %>% fct_relevel("PCR Replicates", "Same Day")

ggplot (all.distances.to.plot) +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance",
        title = "Kasilof") +
    guides (fill = "none")
```

next i will follow what was done here:  (https://github.com/ramongallego/eDNA.and.Ocean.Acidification.Gallego.et.al.2020/blob/master/Scripts/Denoising.all.runs.Rmd) and instead of choosing outliers based on the pairwise distances, we can do a similar thing using the distance to centroid. 


now identify and discard outliers 
```{r message=FALSE, warning=FALSE}
normalized_kenai %>%
  group_by(biological_ID) %>% nest() -> nested.cleaning 

nested.cleaning %>% 
  mutate(matrix = map(data, tibble_to_matrix)) -> nested.cleaning

nested.cleaning %>% mutate(ncomparisons = map(matrix, length)) -> nested.cleaning
```

```{r}
dist_to_centroid <- function (x,y) {
  
  #biol <- rep(y, dim(x)[[1]])
  biol <- rep(y, length(x))
  
  if (length(biol) == 1) {
    output = rep(x[1]/2,2)
    names(output) <- attr(x, "Labels")
  }else{ 
    
  dispersion <- betadisper(x, group = biol)
  output = dispersion$distances
  }
  output
    }
```

```{r}
nested.cleaning.temp <- nested.cleaning %>% 
  mutate(distances = map2(matrix, biological_ID, dist_to_centroid))

all_distances <- nested.cleaning.temp %>%
  unnest_longer(distances) %>%
  dplyr::select(biological_ID, distances_id, distances)

hist(all_distances$distances)
```

calculate normal distribution of distances to centroid - NOPE

filter >0.6 distances
```{r}
#normparams <- MASS::fitdistr(all_distances$distances, "normal")$estimate                                      
#probs <- pnorm(all_distances$distances, normparams[1], normparams[2])
#outliers_centroid <- which(probs>0.99)

#discard_centroid <- all_distances$distances_id[outliers_centroid]

discard_centroid_kenai <- all_distances %>%
  filter(distances > 0.6)
discard_centroid_kenai
```
which extraction/bottle ID have a pcr replicate that's recommended for removal? 
```{r}
#to_discard <- data.frame(discard_centroid) %>%
#  separate(discard_centroid, into = c("days_season", "biological_ID", "replicate"))

#write.csv(to_discard, "20230921_pcr_outliers.csv")

removed_dissim <- normalized_kenai %>%
  filter(biological_ID %in% discard_centroid_kenai$biological_ID)
```


these samples have at least one dissimilar pcr replicates 
```{r}
unique(removed_dissim$biological_ID)

#first_six <- unique(removed_step5$extraction_ID)[1:6]
#first_three <- unique(removed_step5$extraction_ID)[1:3]

removed_dissim %>%
  #filter(extraction_ID %in% first_three) %>%
  filter(tot_reads > 0) %>%
  group_by(sample_ID) %>%
  mutate(sum=sum(tot_reads)) %>%
  mutate(prop = tot_reads/sum) %>%
  ggplot(aes(x=sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~biological_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    #legend.position = "none",
    legend.title = element_blank()
  )  
```


now identify and discard outliers 
```{r message=FALSE, warning=FALSE}
normalized_kasilof %>%
  group_by(biological_ID) %>% nest() -> nested.cleaning 

nested.cleaning %>% 
  mutate(matrix = map(data, tibble_to_matrix)) -> nested.cleaning

nested.cleaning %>% mutate(ncomparisons = map(matrix, length)) -> nested.cleaning
```

```{r}
nested.cleaning.temp <- nested.cleaning %>% 
  mutate(distances = map2(matrix, biological_ID, dist_to_centroid))

all_distances <- nested.cleaning.temp %>%
  unnest_longer(distances) %>%
  dplyr::select(biological_ID, distances_id, distances)

hist(all_distances$distances)
```

calculate normal distribution of distances to centroid
```{r}
# normparams <- MASS::fitdistr(all_distances$distances, "normal")$estimate                                      
# probs <- pnorm(all_distances$distances, normparams[1], normparams[2])
# outliers_centroid <- which(probs>0.99)
# 
# discard_centroid <- all_distances$distances_id[outliers_centroid]
# discard_centroid

discard_centroid_kasilof <- all_distances %>%
  filter(distances > 0.6)
discard_centroid_kasilof
```

none for kasilof 

okay, just remove the one pcr rep from the kenai from the normalized taxon table 

```{r}
normalized_filtered <- normalized %>%
  filter(!day_bottle_pcr %in% discard_centroid_kenai$distances_id) %>%
  rename(reads = tot_reads) %>%
  select(!day_bottle_pcr) %>%
  select(!day_bottle)
```

only field samples remain in this dataset. 

### Step 7. Pool the data from technical replicates and take mean proportions across biological replicates (aka bottles of water)

create simplified dataframe to calculate the eDNA index using the Wisconsin double-standardization from vegan
```{r}
sample_df <- normalized_filtered %>%
  unite(location_day, c("location", "days_season"), sep = "_") %>%
  select(biological_ID, taxon, reads, location_day) %>%
  group_by(location_day, taxon, biological_ID) %>%
  summarize(sumreads = sum(reads)) %>% ## sum the technical (pcr) replicates
  group_by(location_day, biological_ID) %>%
  mutate(Tot = sum(sumreads),
              Row.prop = sumreads / Tot)  %>% ## this creates the proportion on each biological replicate
  group_by(location_day) %>%
  mutate(nreps = length(unique(biological_ID))) %>%
  group_by(location_day, taxon) %>%
  summarise (mean.prop = sum (Row.prop) / max(nreps)) %>%
  pivot_wider(names_from = taxon, values_from = mean.prop)

ids <- sample_df$location_day

sample_df <- sample_df[,-1]

wis_index <- wisconsin(sample_df)

rowSums(wis_index)

wis_index$location_day <- ids

wis_index_long <- wis_index %>%
  pivot_longer(cols = c(1:36), names_to = "taxon", values_to = "normalized")
```

output this table for multivariate community analyses
```{r}
write.csv(wis_index, "~/kenai_mifish_mb/outputs/taxon_index.csv", row.names = F)
```


### Step 8. make a few simple compositional barplots

```{r}
my_df <- sample_metadata %>%
  filter(sample_type == "sample") %>%
  select(location, days_season, collection_date) %>%
  unite(location_day, "location", "days_season", sep = "_", remove = F) %>%
  unique() %>%
  left_join(wis_index_long, by = "location_day")
```

output long version of data w/sampling date included
```{r}
write.csv(my_df, "~/kenai_mifish_mb/outputs/taxon_index_long.csv", row.names = F)
```

plot kenai 
```{r}
plot_kenai <- my_df %>%
  filter(location == "Kenai") %>%
  filter(normalized > 0) %>%
  group_by(location_day) %>%
  #mutate(sum=sum(reads)) %>%
  #mutate(prop = reads/sum) %>%
  ggplot(aes(x=as.factor(days_season), y=normalized, fill=taxon)) +     ### check if i should be plotting mean proportions (pre-wis index)
  geom_bar(stat = "identity") + 
  theme_bw() +
  labs(
    y = "index of abundance",
    x = "sampling day",
    title = "Kenai") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    #legend.position = "none",
    legend.title = element_blank()
  )
plot_kenai
```


plot kasilof 
```{r}
plot_kasilof <- my_df %>%
  filter(location == "Kasilof") %>%
  filter(normalized > 0) %>%
  group_by(location_day) %>%
  #mutate(sum=sum(reads)) %>%
  #mutate(prop = reads/sum) %>%
  ggplot(aes(x=as.factor(days_season), y=normalized, fill=taxon)) +     ### check if i should be plotting mean proportions (pre-wis index)
  geom_bar(stat = "identity") + 
  theme_bw() +
  labs(
    y = "index of abundance",
    x = "sampling day",
    title = "Kasilof") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    #legend.position = "none",
    legend.title = element_blank()
  )
plot_kasilof 
```

```{r}
ggsave(plot = plot_kenai, "/home/kimberly.ledger/kenai_mifish_mb/outputs/kenai.png", width = 10, height =5)
ggsave(plot = plot_kasilof, "/home/kimberly.ledger/kenai_mifish_mb/outputs/kasilof.png", width = 8, height = 5)
```
